{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ece00c",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "This notebook provides some guidance to download the data, set up the directories and launch the registry creation for a given departement. It assumes that the environment has been created. It is still under construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c0af5",
   "metadata": {},
   "source": [
    "## 1. Downloads\n",
    "\n",
    "The data needed is the following : \n",
    "\n",
    "- Images from the IGN\n",
    "- Topological data\n",
    "- Geographical coordinates of the cities\n",
    "\n",
    "Besides, we need a classification and a segmentation model\n",
    "\n",
    "\n",
    "#### Image data\n",
    "\n",
    "#### Topological data\n",
    "\n",
    "#### Geographical coordinates of the cities\n",
    "\n",
    "#### Model weights\n",
    "\n",
    "<b> Include the RF model weights </b>\n",
    "\n",
    "Once the models are downloaded, put them in the `model` directory. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc906c2e",
   "metadata": {},
   "source": [
    "## 2. Setting up the data and model directories \n",
    "\n",
    "Once the data is downloaded, we need to specify in which directories these files are located. In the cell below, input the local directories. At the end, the `config.yml` file will be automatically edited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a700d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "\n",
    "images_directory = ''\n",
    "topological_data_directory = ''\n",
    "geographical_coordinates = ''\n",
    "\n",
    "model_dir = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e40c8",
   "metadata": {},
   "source": [
    "## 3. Setting up the working directories and the parameters \n",
    "\n",
    "The pipeline is comprised of three parts : classification, segmentation and aggregation. \n",
    "\n",
    "- During classification : tile images are cut into thumbnails and images that contain a PV panel are stored in a dedicated folder\n",
    "- During segmentation : images from the dedicated folder are segmented to delineate the PV panels. The segmentation masks are then converted as polygons and stored in a `geojson` file, located in the `data` folder\n",
    "- During aggregation, PV panels characteristics are extracted. The extraction method is specified by the user.\n",
    "\n",
    "\n",
    "### 3.1. Setting up the working directories\n",
    "\n",
    "These directories are the directories in which the intermediary files and the final outputs will be stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "834aeda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dir = 'data' # directory that stores the outputs of the model\n",
    "aux_dir = 'aux' # directory that stores the auxiliary outputs used for inference\n",
    "temp_dir = 'temp' # directory in which the temporary outputs are stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acea870",
   "metadata": {},
   "source": [
    "### 3.2. Choosing which parts of the pipeline we want to execute\n",
    "\n",
    "First of all, we need to choose which parts of the pipeline we want to execute. Since we are lauching the pipeline for the first time, we run all parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0f0e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_classification = True\n",
    "run_segmentation = True\n",
    "run_aggregation = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7290964",
   "metadata": {},
   "source": [
    "### 3.3. Setting up the parameters\n",
    "\n",
    "Each part of the pipeline requires some parameters to be executed. \n",
    "\n",
    "#### Classification and segmentation\n",
    "\n",
    "- `patch_size` : the size of the thumbnail that is passed into the classification and segmentation models\n",
    "- `device` : the device on which inference (classification and segmentation) will be made\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a3323c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 299 # Assuming one is using the joined pretrained models\n",
    "device = 'cuda' # assuming you have a GPU. Else, replace with 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b476a",
   "metadata": {},
   "source": [
    "#### Classification \n",
    "\n",
    "- `cls_batch_size` : the number of samples to be processed at the same time\n",
    "- `cls_threshold` : the classification threshold (above = PV panel, below = no PV panel)\n",
    "- `cls_model` : the name of the classification model, located in the `models_dir` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "846d4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_batch_size = 512 # Depends on your available VRAM\n",
    "cls_threshold = 0.4 # assuming you're using the default model\n",
    "cls_model = \"model_bdappv_cls\" # or replace with your own model name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7562e87b",
   "metadata": {},
   "source": [
    "#### Segmentation\n",
    "\n",
    "- `seg_threshold` : the segmentation threshold\n",
    "- `num_gpu` : the number of GPUs to be used. Depends on your infrastructure.\n",
    "- `seg_batch_size` : the batch size for segmentation. Depends on your available VRAM\n",
    "- `seg_model` : the name of the segmentation model in the `models_dir` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "495fe06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_threshold : 0.46\n",
    "num_gpu : 1\n",
    "seg_batch_size : 128\n",
    "seg_model : 'model_bdappv_seg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c423025",
   "metadata": {},
   "source": [
    "#### Aggregation\n",
    "\n",
    "- `filter_building` : whether the polygons should be matched with a building \n",
    "- `filter_LUT` : True : whether the tilt is inputed using the look up table\n",
    "- `constant_kWp` : False : whether the installed capacity is estimated from the surface area using a linear regression model or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d005113",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_building = True \n",
    "filter_LUT = True # update to replace with the random forest, which is located in the /models directory\n",
    "constant_kWp = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648fef03",
   "metadata": {},
   "source": [
    "Finally, we edit the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the config file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664fc307",
   "metadata": {},
   "source": [
    "## 4. A first run\n",
    "\n",
    "If you are doing inference on a departement for the first time, you <b> first need to run the `auxiliary.py` script </b>. This script generate the auxiliary data that is then used throughout the main pipeline. To run this script, enter the departement number and execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fd345bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpt = 69\n",
    "# ./auxiliary.py --dpt=dpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6241ef",
   "metadata": {},
   "source": [
    "Once the auxiliary files have been generated, the main pipeline can be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9da73478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./main.py --dpt=dpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df8707",
   "metadata": {},
   "source": [
    "This is it ! We mapped all installations for our target departement. The files generated are the following : \n",
    "- `arrays_{dpt}.geojson` : the file with all segmentation polygons obtained at the end of the segmentation stage.\n",
    "- `arrays_characteristics_{dpt}.geojson` : a file with the polygons of all installations after filtering. The characteristics of the polygons are also reported\n",
    "- `characteristics_{dpt}.csv` : a file where each row is an installation. Contains the characteristics and the localization of the installation.\n",
    "- `aggregated_characterisitcs_{dpt}.csv` : aggregates the installed capacity and number of installations per city \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36202a06",
   "metadata": {},
   "source": [
    "## 5. Monitoring the accuracy of the detections\n",
    "\n",
    "Now that we've detected our installations, we want to measure the accuracy of the estimates. To do so, we first need to download the <i> registre national d'installations </i> (RNI). The RNI aggregates the installed capacity of installations below 36 kWc by city. When evaluating our outputs, we compare our estimates with this reference. \n",
    "\n",
    "#### Downloading the RNI\n",
    "\n",
    "The RNI can be accessed here. Since in this example we are working with images released in 2020, we download the RNI for 2020. Other years can be accessed here (2017, 2018, 2019, 2021). \n",
    "\n",
    "#### Setting up the directory and running the evaluation script\n",
    "\n",
    "Put the RNI in the `source_dir` and input its name in the cell below. Then, execute the cell to run the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "200136b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'RNI_2020.json'\n",
    "source_dir = '' # enter your path to the RNI here.\n",
    "\n",
    "# edit the configuration file\n",
    "\n",
    "# run the evaluation\n",
    "# ./evaluate.py --dpt=dpt --filename=filename --source_dir=source_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f529a0",
   "metadata": {},
   "source": [
    "The outputs are located in the newly created `evaluation_dir` directory. If you want to visualize your outputs, use the `visualization.ipynb` notebook !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464541db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfrance",
   "language": "python",
   "name": "dsfrance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
